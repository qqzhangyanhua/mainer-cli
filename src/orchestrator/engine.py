"""ReAct å¾ªç¯å¼•æ“"""

from __future__ import annotations

import inspect
from collections.abc import Awaitable, Callable
from typing import Optional

from typing import TYPE_CHECKING

from src.config.manager import OpsAIConfig
from src.context.environment import EnvironmentContext
from src.llm.client import LLMClient

from src.orchestrator.error_helper import ErrorHelper
from src.orchestrator.graph_adapter import build_graph_messages, parse_graph_messages
from src.orchestrator.instruction import (
    available_workers_text,
    build_fallback_instruction,
    generate_instruction_with_retry,
)
from src.orchestrator.preprocessor import RequestPreprocessor
from src.orchestrator.prompt import PromptBuilder
from src.orchestrator.safety import check_safety
from src.orchestrator.validation import validate_instruction
from src.types import ConversationEntry, Instruction, RiskLevel, WorkerResult
from src.workers.audit import AuditWorker
from src.workers.base import BaseWorker
from src.workers.system import SystemWorker

if TYPE_CHECKING:
    from src.orchestrator.graph import ReactGraph


class OrchestratorEngine:
    """Orchestrator å¼•æ“

    å®ç° ReAct (Reason-Act) å¾ªç¯ï¼š
    1. Reason: LLM ç”Ÿæˆä¸‹ä¸€æ­¥æŒ‡ä»¤
    2. Safety Check: æ£€æŸ¥å®‰å…¨çº§åˆ«
    3. Act: æ‰§è¡Œ Worker
    4. åˆ¤æ–­æ˜¯å¦å®Œæˆ
    """

    def __init__(
        self,
        config: OpsAIConfig,
        confirmation_callback: Optional[
            Callable[[Instruction, RiskLevel], bool | Awaitable[bool]]
        ] = None,
        dry_run: bool = False,
        progress_callback: Optional[Callable[[str, str], None]] = None,
        use_langgraph: bool = False,
        use_sqlite_checkpoint: bool = False,
    ) -> None:
        """åˆå§‹åŒ–å¼•æ“

        Args:
            config: é…ç½®å¯¹è±¡
            confirmation_callback: ç¡®è®¤å›è°ƒå‡½æ•°ï¼Œç”¨äºé«˜å±æ“ä½œç¡®è®¤
            dry_run: æ˜¯å¦å¯ç”¨ dry-run æ¨¡å¼
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°ï¼Œæ¥æ”¶ (step_name, message) ç”¨äºå®æ—¶æ˜¾ç¤ºè¿›åº¦
            use_langgraph: æ˜¯å¦ä½¿ç”¨ LangGraph æ¨¡å¼ï¼ˆé»˜è®¤ Falseï¼Œä¿æŒå‘åå…¼å®¹ï¼‰
            use_sqlite_checkpoint: æ˜¯å¦ä½¿ç”¨ SQLite æŒä¹…åŒ–æ£€æŸ¥ç‚¹ï¼ˆä»…å½“ use_langgraph=True æ—¶æœ‰æ•ˆï¼‰
        """
        self._config = config
        self._llm_client = LLMClient(config.llm)
        self._prompt_builder = PromptBuilder()
        self._preprocessor = RequestPreprocessor()
        self._error_helper = ErrorHelper()
        self._context = EnvironmentContext()
        self._confirmation_callback = confirmation_callback
        self._dry_run = dry_run or config.safety.dry_run_by_default
        self._progress_callback = progress_callback
        self._use_langgraph = use_langgraph

        # åˆå§‹åŒ– Workers
        self._workers: dict[str, BaseWorker] = {
            "system": SystemWorker(),
            "audit": AuditWorker(),
        }

        # æ³¨å†Œ ChatWorker
        try:
            from src.workers.chat import ChatWorker

            self._workers["chat"] = ChatWorker()
        except ImportError:
            pass

        # æ³¨å†Œ ShellWorker
        try:
            from src.workers.shell import ShellWorker

            self._workers["shell"] = ShellWorker()
        except ImportError:
            pass

        # å°è¯•å¯¼å…¥å¹¶æ³¨å†Œ ContainerWorker
        try:
            from src.workers.container import ContainerWorker

            self._workers["container"] = ContainerWorker()
        except ImportError:
            pass

        # æ³¨å†Œ AnalyzeWorkerï¼ˆéœ€è¦ LLM å®¢æˆ·ç«¯ï¼‰
        try:
            from src.workers.analyze import AnalyzeWorker

            self._workers["analyze"] = AnalyzeWorker(self._llm_client)
        except ImportError:
            pass

        # æ³¨å†Œ HttpWorker
        try:
            from src.workers.http import HttpWorker

            self._workers["http"] = HttpWorker(self._config.http)
        except ImportError:
            pass

        # æ³¨å†Œ GitWorker
        try:
            from src.workers.git import GitWorker

            self._workers["git"] = GitWorker()
        except ImportError:
            pass

        # æ³¨å†Œ DeployWorkerï¼ˆéœ€è¦ HttpWorkerã€ShellWorker å’Œ LLMClientï¼‰
        http_worker = self._workers.get("http")
        shell_worker = self._workers.get("shell")
        if http_worker and shell_worker:
            try:
                from src.workers.deploy import DeployWorker
                from src.workers.http import HttpWorker as HttpWorkerType
                from src.workers.shell import ShellWorker as ShellWorkerType

                if isinstance(http_worker, HttpWorkerType) and isinstance(
                    shell_worker, ShellWorkerType
                ):
                    # åˆ›å»ºé€‚é…å™¨ï¼šå°† DeployWorker çš„ç¡®è®¤å›è°ƒé€‚é…åˆ° Engine çš„ç¡®è®¤å›è°ƒ
                    deploy_confirmation_callback = None
                    deploy_ask_user_callback = None

                    if confirmation_callback is not None:
                        deploy_confirmation_callback = self._create_deploy_confirmation_adapter(
                            confirmation_callback
                        )
                        # ask_user_callback éœ€è¦ä»å¤–éƒ¨æ³¨å…¥ï¼Œå…ˆè®¾ä¸º None
                        # DeployWorker æ”¯æŒåç»­é€šè¿‡ set_ask_user_callback æ³¨å…¥

                    self._workers["deploy"] = DeployWorker(
                        http_worker,
                        shell_worker,
                        self._llm_client,  # ä¼ é€’ LLM å®¢æˆ·ç«¯å®ç°æ™ºèƒ½éƒ¨ç½²
                        progress_callback,  # ä¼ é€’è¿›åº¦å›è°ƒ
                        deploy_confirmation_callback,  # ä¼ é€’ç¡®è®¤å›è°ƒï¼ˆé€‚é…å™¨ï¼‰
                        deploy_ask_user_callback,  # ç”¨æˆ·é€‰æ‹©å›è°ƒï¼ˆåç»­æ³¨å…¥ï¼‰
                    )
            except ImportError:
                pass

        # åˆå§‹åŒ– ReactGraphï¼ˆå¦‚æœå¯ç”¨ï¼‰
        self._react_graph: Optional["ReactGraph"] = None
        if self._use_langgraph:
            from src.orchestrator.graph import ReactGraph

            # åˆ¤æ–­æ˜¯å¦å¯ç”¨ interruptï¼ˆTUI æ¨¡å¼æ‰éœ€è¦ï¼‰
            enable_interrupts = confirmation_callback is not None
            self._react_graph = ReactGraph(
                llm_client=self._llm_client,
                workers=self._workers,
                context=self._context,
                dry_run=self._dry_run,
                enable_checkpoints=True,
                enable_interrupts=enable_interrupts,
                use_sqlite=use_sqlite_checkpoint,
                checkpoint_db_path=None,  # ä½¿ç”¨é»˜è®¤è·¯å¾„
                progress_callback=progress_callback,
            )

    def get_worker(self, name: str) -> Optional[BaseWorker]:
        """è·å– Worker

        Args:
            name: Worker åç§°

        Returns:
            Worker å®ä¾‹ï¼Œä¸å­˜åœ¨è¿”å› None
        """
        return self._workers.get(name)

    def _create_deploy_confirmation_adapter(
        self,
        confirmation_callback: Callable[[Instruction, RiskLevel], bool | Awaitable[bool]],
    ) -> Callable[[str, str], Awaitable[bool]]:
        """åˆ›å»º DeployWorker ç¡®è®¤å›è°ƒçš„é€‚é…å™¨

        å°† DeployWorker çš„ (action, detail) æ ¼å¼è½¬æ¢ä¸º Engine çš„ (Instruction, RiskLevel) æ ¼å¼
        """

        async def adapter(action: str, detail: str) -> bool:
            # åˆ›å»ºä¸€ä¸ªè™šæ‹Ÿçš„ Instruction ç”¨äºç¡®è®¤å¯¹è¯æ¡†
            instruction = Instruction(
                worker="deploy",
                action="è‡ªä¸»ä¿®å¤",
                args={"operation": action, "detail": detail},
                risk_level="medium",
            )
            result = confirmation_callback(instruction, "medium")
            if inspect.isawaitable(result):
                return await result
            return bool(result)

        return adapter

    def _get_list_command(self, target_type: str) -> str:
        """æ ¹æ®ç›®æ ‡ç±»å‹è¿”å›åˆ—è¡¨å‘½ä»¤

        Args:
            target_type: å¯¹è±¡ç±»å‹ï¼ˆdockerã€processã€port ç­‰ï¼‰

        Returns:
            å¯¹åº”çš„åˆ—è¡¨å‘½ä»¤
        """
        commands = {
            "docker": "docker ps",
            "process": "ps aux",
            "port": "ss -tlnp",
            "file": "ls -la",
            "systemd": "systemctl list-units --type=service --state=running",
            "network": "ip addr",
        }
        return commands.get(target_type, "docker ps")

    def _available_workers_text(self) -> str:
        """æ„å»ºå¯ç”¨ Worker/Action åˆ—è¡¨æ–‡æœ¬"""
        return available_workers_text(self._workers)

    def _build_instruction(self, parsed: dict[str, object]) -> Instruction:
        """ä»è§£æåçš„ JSON æ„å»ºæŒ‡ä»¤ï¼Œå¸¦åŸºç¡€å®¹é”™ï¼ˆå§”æ‰˜åˆ° instruction æ¨¡å—ï¼‰"""
        from src.orchestrator.instruction import build_instruction
        return build_instruction(parsed)

    def _build_fallback_instruction(
        self, user_input: str, error_message: str
    ) -> Optional[Instruction]:
        """æ„å»ºå…œåº•æŒ‡ä»¤ï¼ˆå§”æ‰˜åˆ° instruction æ¨¡å—ï¼‰"""
        return build_fallback_instruction(user_input, error_message, self._workers)

    def _parse_and_validate_instruction(self, response: str) -> tuple[Optional[Instruction], str]:
        """è§£æå¹¶æ ¡éªŒ LLM æŒ‡ä»¤ï¼ˆå§”æ‰˜åˆ° instruction æ¨¡å—ï¼‰"""
        from src.orchestrator.instruction import parse_and_validate_instruction
        return parse_and_validate_instruction(response, self._llm_client, self._workers)

    def _build_repair_prompt(self, user_input: str, error_message: str) -> str:
        """æ„å»ºä¿®å¤æç¤ºï¼ˆå§”æ‰˜åˆ° instruction æ¨¡å—ï¼‰"""
        from src.orchestrator.instruction import build_repair_prompt
        return build_repair_prompt(user_input, error_message, self._workers)

    async def _generate_instruction_with_retry(
        self,
        system_prompt: str,
        user_prompt: str,
        user_input: str,
        history: Optional[list[ConversationEntry]],
    ) -> tuple[Optional[Instruction], str]:
        """ç”ŸæˆæŒ‡ä»¤å¹¶è¿›è¡Œä¸€æ¬¡çº é”™é‡è¯•ï¼ˆå§”æ‰˜åˆ° instruction æ¨¡å—ï¼‰"""
        return await generate_instruction_with_retry(
            self._llm_client, self._workers,
            system_prompt, user_prompt, user_input, history,
        )

    def _build_graph_messages(
        self, history: Optional[list[ConversationEntry]]
    ) -> list[dict[str, object]]:
        """å°† ConversationEntry è½¬æ¢ä¸º LangGraph æ¶ˆæ¯æ ¼å¼ï¼ˆå§”æ‰˜åˆ° graph_adapterï¼‰"""
        return build_graph_messages(history)

    def _parse_graph_messages(self, messages: list[object]) -> list[ConversationEntry]:
        """ä» LangGraph æ¶ˆæ¯å†å²è§£æ ConversationEntryï¼ˆå§”æ‰˜åˆ° graph_adapterï¼‰"""
        return parse_graph_messages(messages)

    async def execute_instruction(self, instruction: Instruction) -> WorkerResult:
        """æ‰§è¡ŒæŒ‡ä»¤

        Args:
            instruction: å¾…æ‰§è¡Œçš„æŒ‡ä»¤

        Returns:
            æ‰§è¡Œç»“æœ
        """
        worker = self.get_worker(instruction.worker)
        if worker is None:
            return WorkerResult(
                success=False,
                message=f"Unknown worker: {instruction.worker}",
            )

        # å¦‚æœå…¨å±€å¯ç”¨äº† dry_runï¼Œåˆ™æ³¨å…¥åˆ°å‚æ•°ä¸­
        args = instruction.args.copy()
        if self._dry_run or instruction.dry_run:
            args["dry_run"] = True

        return await worker.execute(instruction.action, args)

    async def react_loop(
        self,
        user_input: str,
        max_iterations: int = 5,
        session_history: Optional[list[ConversationEntry]] = None,
    ) -> str:
        """æ‰§è¡Œ ReAct å¾ªç¯

        Args:
            user_input: ç”¨æˆ·è¾“å…¥
            max_iterations: æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼Œé˜²æ­¢æ­»å¾ªç¯
            session_history: ä¼šè¯çº§å¯¹è¯å†å²ï¼ˆè·¨è½®æ¬¡ä¿æŒï¼‰

        Returns:
            æœ€ç»ˆç»“æœæ¶ˆæ¯
        """
        # ä½¿ç”¨ä¼ å…¥çš„ä¼šè¯å†å²ï¼Œæˆ–åˆ›å»ºæ–°çš„
        # æ³¨æ„ï¼šå¿…é¡»ä¿æŒå¼•ç”¨ï¼Œä¸èƒ½ç”¨ `or []`ï¼ˆç©ºåˆ—è¡¨è¢«è§†ä¸º falsyï¼‰
        conversation_history: list[ConversationEntry] = (
            session_history if session_history is not None else []
        )

        for iteration in range(max_iterations):
            # 0. é¢„å¤„ç†ï¼šæ„å›¾æ£€æµ‹ + æŒ‡ä»£è§£æ
            preprocessed = self._preprocessor.preprocess(user_input, conversation_history)

            # é«˜ç½®ä¿¡åº¦çš„è§£é‡Šæ„å›¾ - ç›´æ¥ç”Ÿæˆ Instructionï¼Œç»•è¿‡ LLM
            if preprocessed.intent == "identity":
                chat_worker = self.get_worker("chat")
                if chat_worker and "respond" in chat_worker.get_capabilities():
                    if self._progress_callback:
                        self._progress_callback("preprocessing", "ğŸ‘‹ Detected: identity request")

                    instruction = Instruction(
                        worker="chat",
                        action="respond",
                        args={
                            "message": (
                                "æˆ‘æ˜¯ä¸€ä¸ªè¿ç»´åŠ©æ‰‹ï¼Œå¯ä»¥å¸®ä½ æ’æŸ¥é—®é¢˜ã€éƒ¨ç½²é¡¹ç›®ã€æŸ¥çœ‹æ—¥å¿—ã€"
                                "æ‰§è¡Œå¸¸ç”¨å‘½ä»¤å¹¶è§£é‡Šè¾“å‡ºã€‚å‘Šè¯‰æˆ‘ä½ çš„éœ€æ±‚å³å¯ã€‚"
                            )
                        },
                        risk_level="safe",
                    )
                else:
                    # æ—  chat workerï¼Œå›é€€åˆ°æ™®é€šæµç¨‹
                    if self._progress_callback:
                        self._progress_callback("reasoning", "ğŸ¤” Analyzing your request...")

                    system_prompt = self._prompt_builder.build_system_prompt(
                        self._context,
                        available_workers=self._workers,
                    )
                    user_prompt = self._prompt_builder.build_user_prompt(user_input, history=None)

                    instruction, error = await self._generate_instruction_with_retry(
                        system_prompt=system_prompt,
                        user_prompt=user_prompt,
                        user_input=user_input,
                        history=conversation_history,
                    )
                    if instruction is None:
                        return f"Error: {error}"
            elif (
                preprocessed.confidence == "high"
                and preprocessed.intent == "explain"
                and preprocessed.resolved_target
            ):
                if self._progress_callback:
                    target = preprocessed.resolved_target
                    ttype = preprocessed.target_type
                    self._progress_callback(
                        "preprocessing",
                        f"ğŸ¯ Detected: explain '{target}' ({ttype})",
                    )

                instruction = Instruction(
                    worker="analyze",
                    action="explain",
                    args={
                        "target": preprocessed.resolved_target,
                        "type": preprocessed.target_type or "docker",
                    },
                    risk_level="safe",
                )
                # è·³è¿‡ LLM æ¨ç†ï¼Œç›´æ¥æ‰§è¡Œ
            elif (
                preprocessed.intent == "explain"
                and preprocessed.needs_context
                and preprocessed.target_type
            ):
                # éœ€è¦å…ˆè·å–ä¸Šä¸‹æ–‡å†åˆ†æ
                # æ ¹æ®ç±»å‹ç”Ÿæˆåˆ—è¡¨å‘½ä»¤
                if self._progress_callback:
                    self._progress_callback(
                        "preprocessing",
                        f"ğŸ” Need context for {preprocessed.target_type}, fetching list first...",
                    )

                list_command = self._get_list_command(preprocessed.target_type)
                instruction = Instruction(
                    worker="shell",
                    action="execute_command",
                    args={"command": list_command},
                    risk_level="safe",
                )
                # task_completed é»˜è®¤ä¸º Falseï¼Œå¾ªç¯ä¼šç»§ç»­
            elif preprocessed.intent == "deploy":
                # deploy æ„å›¾ - ç›´æ¥ä½¿ç”¨ä¸€é”®éƒ¨ç½²ï¼Œæ— éœ€åˆ†æ­¥
                repo_url = self._preprocessor.extract_repo_url(user_input)
                if repo_url and self.get_worker("deploy"):
                    if self._progress_callback:
                        self._progress_callback(
                            "preprocessing", f"ğŸš€ Deploy intent detected for: {repo_url}"
                        )

                    # ç›´æ¥ç”Ÿæˆä¸€é”®éƒ¨ç½²æŒ‡ä»¤ï¼Œä¸å†ä½¿ç”¨åˆ†æ­¥æµç¨‹
                    instruction = Instruction(
                        worker="deploy",
                        action="deploy",
                        args={"repo_url": repo_url, "target_dir": "~/projects"},
                        risk_level="medium",
                    )
                else:
                    # æ— æ³•æå– URL æˆ–ç¼ºå°‘ deploy workerï¼Œå›é€€åˆ°æ™®é€šå¤„ç†
                    if self._progress_callback:
                        self._progress_callback("reasoning", "ğŸ¤” Analyzing your request...")

                    system_prompt = self._prompt_builder.build_system_prompt(
                        self._context,
                        available_workers=self._workers,
                    )
                    user_prompt = self._prompt_builder.build_user_prompt(user_input, history=None)

                    instruction, error = await self._generate_instruction_with_retry(
                        system_prompt=system_prompt,
                        user_prompt=user_prompt,
                        user_input=user_input,
                        history=conversation_history,
                    )
                    if instruction is None:
                        return f"Error: {error}"
            else:
                # 1. Reason: LLM ç”Ÿæˆä¸‹ä¸€æ­¥æŒ‡ä»¤
                if self._progress_callback:
                    self._progress_callback("reasoning", "ğŸ¤” Analyzing your request...")

                system_prompt = self._prompt_builder.build_system_prompt(
                    self._context,
                    available_workers=self._workers,
                )
                # ä¸å†åœ¨ user_prompt ä¸­åµŒå…¥å†å²ï¼Œæ”¹ç”¨ LLM æ ‡å‡†å¤šè½®å¯¹è¯æ ¼å¼
                user_prompt = self._prompt_builder.build_user_prompt(user_input, history=None)

                instruction, error = await self._generate_instruction_with_retry(
                    system_prompt=system_prompt,
                    user_prompt=user_prompt,
                    user_input=user_input,
                    history=conversation_history,
                )
                if instruction is None:
                    return f"Error: {error}"

            # æŒ‡ä»¤æ ¡éªŒï¼ˆé˜²æ­¢æœªçŸ¥ Worker/Actionï¼‰
            valid, error = validate_instruction(instruction, self._workers)
            if not valid:
                fallback = self._build_fallback_instruction(user_input, error)
                if fallback:
                    instruction = fallback
                else:
                    return f"Error: {error}"

            # æ˜¾ç¤ºç”Ÿæˆçš„æŒ‡ä»¤
            if self._progress_callback:
                worker_action = f"{instruction.worker}.{instruction.action}"
                self._progress_callback(
                    "instruction",
                    f"ğŸ“‹ Instruction: {worker_action}(args={instruction.args})",
                )

            # 2. Safety Check
            risk = check_safety(instruction)
            if self._progress_callback:
                risk_emoji = {"safe": "âœ…", "medium": "âš ï¸", "high": "ğŸš¨"}.get(risk, "â“")
                self._progress_callback("safety", f"{risk_emoji} Risk level: {risk}")
            if risk in ["medium", "high"]:
                if self._confirmation_callback:
                    confirmed = self._confirmation_callback(instruction, risk)
                    if inspect.isawaitable(confirmed):
                        confirmed = await confirmed
                    if not confirmed:
                        # è®°å½•æ‹’ç»
                        await self._log_operation(
                            user_input,
                            instruction,
                            risk,
                            confirmed=False,
                            exit_code=-1,
                            output="Rejected by user",
                        )
                        return "Operation cancelled by user"
                else:
                    # CLI æ¨¡å¼æ— ç¡®è®¤å›è°ƒï¼Œè‡ªåŠ¨æ‹’ç»
                    return (
                        f"Error: {risk.upper()}-risk operation requires TUI mode for confirmation"
                    )

            # 3. Act: æ‰§è¡Œ Worker
            if self._progress_callback:
                self._progress_callback(
                    "executing", f"âš™ï¸  Executing {instruction.worker}.{instruction.action}..."
                )

            result = await self.execute_instruction(instruction)

            # å¦‚æœå¤±è´¥ï¼Œå¢å¼ºé”™è¯¯æ¶ˆæ¯
            if not result.success:
                result = self._error_helper.enhance_error_message(result, user_input)

            if self._progress_callback:
                status_emoji = "âœ…" if result.success else "âŒ"
                self._progress_callback("result", f"{status_emoji} {result.message}")

            # 4. è®°å½•åˆ°å®¡è®¡æ—¥å¿—
            await self._log_operation(
                user_input,
                instruction,
                risk,
                confirmed=True,
                exit_code=0 if result.success else 1,
                output=result.message,
            )

            # 5. è®°å½•å†å²ï¼ˆåŒ…å«ç”¨æˆ·åŸå§‹è¾“å…¥ï¼‰
            conversation_history.append(
                ConversationEntry(
                    instruction=instruction,
                    result=result,
                    user_input=user_input,
                )
            )

            # 6. åˆ¤æ–­æ˜¯å¦å®Œæˆ
            if result.task_completed:
                return result.message

        return "Task incomplete: reached maximum iterations"

    async def react_loop_graph(
        self,
        user_input: str,
        max_iterations: int = 5,
        session_id: Optional[str] = None,
        session_history: Optional[list[ConversationEntry]] = None,
    ) -> str:
        """æ‰§è¡Œ ReAct å¾ªç¯ï¼ˆLangGraph ç‰ˆæœ¬ï¼‰

        Args:
            user_input: ç”¨æˆ·è¾“å…¥
            max_iterations: æœ€å¤§è¿­ä»£æ¬¡æ•°
            session_id: ä¼šè¯ IDï¼ˆç”¨äºæŒä¹…åŒ–å’Œæ¢å¤ï¼‰

        Returns:
            æœ€ç»ˆç»“æœæ¶ˆæ¯
        """
        if self._react_graph is None:
            return "Error: LangGraph mode not enabled. Set use_langgraph=True in constructor."

        try:
            messages = self._build_graph_messages(session_history)
            final_state = await self._react_graph.run(
                user_input=user_input,
                session_id=session_id,
                max_iterations=max_iterations,
                messages=messages,
            )

            # æ£€æŸ¥æ˜¯å¦è¢«ä¸­æ–­ï¼ˆéœ€è¦å®¡æ‰¹ï¼‰
            if final_state.get("needs_approval") and not final_state.get("approval_granted"):
                # è¿”å›ç‰¹æ®Šæ¶ˆæ¯ï¼ŒTUI å¯ä»¥æ®æ­¤åˆ¤æ–­éœ€è¦è°ƒç”¨ resume_react_loop
                return "__APPROVAL_REQUIRED__"

            # æ›´æ–°ä¼šè¯å†å²
            if session_history is not None:
                parsed = self._parse_graph_messages(final_state.get("messages", []))
                session_history.clear()
                session_history.extend(parsed)

            return final_state.get("final_message", "Task completed")
        except Exception as e:
            return f"Error in ReactGraph: {e}"

    async def resume_react_loop(
        self,
        session_id: str,
        approval_granted: bool = True,
        session_history: Optional[list[ConversationEntry]] = None,
    ) -> str:
        """æ¢å¤è¢«ä¸­æ–­çš„ ReAct å¾ªç¯ï¼ˆå®¡æ‰¹åç»§ç»­ï¼‰

        Args:
            session_id: ä¼šè¯ ID
            approval_granted: å®¡æ‰¹æ˜¯å¦é€šè¿‡

        Returns:
            æœ€ç»ˆç»“æœæ¶ˆæ¯
        """
        if self._react_graph is None:
            return "Error: LangGraph mode not enabled"

        try:
            final_state = await self._react_graph.resume(
                session_id=session_id,
                approval_granted=approval_granted,
            )

            if final_state.get("needs_approval") and not final_state.get("approval_granted"):
                return "__APPROVAL_REQUIRED__"

            # æ›´æ–°ä¼šè¯å†å²
            if session_history is not None:
                parsed = self._parse_graph_messages(final_state.get("messages", []))
                session_history.clear()
                session_history.extend(parsed)

            return final_state.get("final_message", "Task completed")
        except Exception as e:
            return f"Error resuming ReactGraph: {e}"

    def get_graph_state(self, session_id: str) -> Optional[dict[str, object]]:
        """è·å– LangGraph ä¼šè¯çŠ¶æ€

        Args:
            session_id: ä¼šè¯ ID

        Returns:
            å½“å‰çŠ¶æ€ï¼Œä¸å­˜åœ¨è¿”å› None
        """
        if self._react_graph is None:
            return None

        return self._react_graph.get_state(session_id)

    def get_mermaid_diagram(self) -> str:
        """è·å– ReAct å·¥ä½œæµçš„ Mermaid å›¾è¡¨

        Returns:
            Mermaid å›¾è¡¨å­—ç¬¦ä¸²
        """
        if self._react_graph is None:
            return "Error: LangGraph mode not enabled"

        return self._react_graph.get_mermaid_diagram()

    async def _log_operation(
        self,
        user_input: str,
        instruction: Instruction,
        risk: RiskLevel,
        confirmed: bool,
        exit_code: int,
        output: str,
    ) -> None:
        """è®°å½•æ“ä½œåˆ°å®¡è®¡æ—¥å¿—"""
        audit_worker = self._workers.get("audit")
        if audit_worker:
            await audit_worker.execute(
                "log_operation",
                {
                    "input": user_input,
                    "worker": instruction.worker,
                    "action": instruction.action,
                    "risk": risk,
                    "confirmed": "yes" if confirmed else "no",
                    "exit_code": exit_code,
                    "output": output,
                },
            )
